<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="符玲玲"><meta name="copyright" content="符玲玲"><meta name="generator" content="Hexo 5.2.0"><meta name="theme" content="hexo-theme-yun"><title>pytorch构建模型 | 玲玲酱的栖息点</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/star-markdown-css@0.1.21/dist/yun/yun-markdown.min.css"><script src="//at.alicdn.com/t/font_1140697_ed8vp4atwoj.js" async></script><script src="https://cdn.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>document.addEventListener("DOMContentLoaded", () => {
  [".post-card",".post-content img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
});
</script><link rel="shortcut icon" type="image/svg+xml" href="/yun.svg"><link rel="mask-icon" href="/yun.svg" color="#0078E7"><link rel="alternate icon" href="/yun.ico"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="preload" href="/js/utils.js" as="script"><link rel="preload" href="/js/hexo-theme-yun.js" as="script"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script id="yun-config">
    const Yun = window.Yun || {};
    window.CONFIG = {"hostname":"lingo1101.github.io","root":"/","title":"玲玲酱の栖息小镇","version":"1.2.0","mode":"auto","copycode":true,"anonymous_image":"https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/avatar/none.jpg","say":{"api":"https://v1.hitokoto.cn","hitokoto":true},"fireworks":{"colors":["102, 167, 221","62, 131, 225","33, 78, 194"]}};
  </script><meta name="description" content="类中的方法和属性方法：也就是各类中定义的函数，比如我们定义一个车的类，描述车的函数就是一个方法。   属性：车的品牌、型号、生产日期等信息就是它的属性. __init__方法：无需直接调用，生成实例对象的时候自动调用。“init”的全称是“initialize”，也就是初始化的意思，所以__init__又称构造方法。   在定义类的时候init()方法是必不可少的。   (1)双下划线开头的函数为">
<meta property="og:type" content="article">
<meta property="og:title" content="pytorch构建模型">
<meta property="og:url" content="https://lingo1101.github.io/2020/10/24/learn/Deeplearn/pytorch/index.html">
<meta property="og:site_name" content="玲玲酱的栖息点">
<meta property="og:description" content="类中的方法和属性方法：也就是各类中定义的函数，比如我们定义一个车的类，描述车的函数就是一个方法。   属性：车的品牌、型号、生产日期等信息就是它的属性. __init__方法：无需直接调用，生成实例对象的时候自动调用。“init”的全称是“initialize”，也就是初始化的意思，所以__init__又称构造方法。   在定义类的时候init()方法是必不可少的。   (1)双下划线开头的函数为">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://lingo1101.github.io/2020/10/24/learn/Deeplearn/pytorch/1.png">
<meta property="og:image" content="https://lingo1101.github.io/2020/10/24/learn/Deeplearn/pytorch/2.png">
<meta property="og:image" content="https://lingo1101.github.io/2020/10/24/learn/Deeplearn/pytorch/3.png">
<meta property="article:published_time" content="2020-10-24T15:05:19.000Z">
<meta property="article:modified_time" content="2020-10-27T02:20:58.280Z">
<meta property="article:author" content="符玲玲">
<meta property="article:tag" content="deeplearn">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://lingo1101.github.io/2020/10/24/learn/Deeplearn/pytorch/1.png"><script src="/js/ui/mode.js"></script></head><body><script defer src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script defer src="/js/ui/fireworks.js"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="Table of Contents"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-list-ordered"></use></svg></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="Overview"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-passport-line"></use></svg></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="符玲玲"><img width="96" loading="lazy" src="/images/fll.jpg" alt="符玲玲"></a><div class="site-author-name"><a href="/about/">符玲玲</a></div><a class="site-name" href="/about/site.html">玲玲酱的栖息点</a><sub class="site-subtitle"></sub><div class="site-desciption">希望能成为一个温柔有趣的人</div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="Home"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-home-4-line"></use></svg></span></a><div class="site-state-item"><a href="/archives/" title="Archives"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-archive-line"></use></svg></span><span class="site-state-item-count">9</span></a></div><div class="site-state-item"><a href="/categories/" title="Categories"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-2-line"></use></svg></span><span class="site-state-item-count">3</span></a></div><div class="site-state-item"><a href="/tags/" title="Tags"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="site-state-item-count">3</span></a></div><a class="site-state-item hty-icon-button" target="_blank" rel="noopener" href="https://yun.yunyoujun.cn" title="文档"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-settings-line"></use></svg></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/Lingo1101" title="GitHub" target="_blank" style="color:#181717"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-github-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://weibo.com/u/5747304475" title="weibo" target="_blank" style="color:#E6162D"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-weibo-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://music.163.com/#/my/m/music/playlist?id=542561628" title="netease-cloud-music" target="_blank" style="color:#C10D0C"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-netease-cloud-music-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="mailto:cathy0923@scuec.edu.cn" title="E-Mail" target="_blank" style="color:#8E71C1"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-mail-line"></use></svg></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-contrast-2-line"></use></svg></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%B1%BB%E4%B8%AD%E7%9A%84%E6%96%B9%E6%B3%95%E5%92%8C%E5%B1%9E%E6%80%A7"><span class="toc-number">1.</span> <span class="toc-text">类中的方法和属性</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#init-%E6%96%B9%E6%B3%95%EF%BC%9A"><span class="toc-number">1.1.</span> <span class="toc-text">__init__方法：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#self%E5%8F%82%E6%95%B0"><span class="toc-number">1.2.</span> <span class="toc-text">self参数</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#pytorch%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA"><span class="toc-number">2.</span> <span class="toc-text">pytorch模型构建</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%80%BB%E4%BD%93%E7%BB%93%E6%9E%84"><span class="toc-number">2.1.</span> <span class="toc-text">模型总体结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89%E7%9A%84%E5%85%B7%E4%BD%93%E6%96%B9%E6%B3%95"><span class="toc-number">2.2.</span> <span class="toc-text">模型定义的具体方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%E4%B8%80%EF%BC%9A"><span class="toc-number">2.2.1.</span> <span class="toc-text">方法一：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%E4%BA%8C%EF%BC%9A"><span class="toc-number">2.2.2.</span> <span class="toc-text">方法二：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%E4%B8%89%EF%BC%9A"><span class="toc-number">2.2.3.</span> <span class="toc-text">方法三：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%E5%9B%9B%EF%BC%9A"><span class="toc-number">2.2.4.</span> <span class="toc-text">方法四：</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%85%B1%E4%BA%AB"><span class="toc-number">3.</span> <span class="toc-text">模型的参数共享</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#ModuleList%E5%92%8CSequencial"><span class="toc-number">4.</span> <span class="toc-text">ModuleList和Sequencial</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#nn-ModuleList"><span class="toc-number">4.1.</span> <span class="toc-text">nn.ModuleList</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#nn-Sequencial"><span class="toc-number">4.2.</span> <span class="toc-text">nn.Sequencial</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#nn-ModuleList%E5%92%8Cnn-Sequential%E4%BD%BF%E7%94%A8%E6%83%85%E6%99%AF"><span class="toc-number">4.3.</span> <span class="toc-text">nn.ModuleList和nn.Sequential使用情景</span></a></li></ol></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="post-block" itemscope itemtype="https://schema.org/Article"><link itemprop="mainEntityOfPage" href="https://Lingo1101.github.io/2020/10/24/learn/Deeplearn/pytorch/"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="符玲玲"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="玲玲酱的栖息点"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">pytorch构建模型</h1><div class="post-meta"><div class="post-time" style="display:inline-block"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-line"></use></svg></span> <time title="Created: 2020-10-24 23:05:19" itemprop="dateCreated datePublished" datetime="2020-10-24T23:05:19+08:00">2020-10-24</time><span class="post-meta-divider">-</span><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-2-line"></use></svg></span> <time title="Modified: 2020-10-27 10:20:58" itemprop="dateModified" datetime="2020-10-27T10:20:58+08:00">2020-10-27</time></div><div class="post-classify"><span class="post-category"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-line"></use></svg></span> <span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category" href="/categories/learn/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">learn</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag" href="/tags/deeplearn/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">deeplearn</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body" style="--smc-primary:#0078E7;"><h1 id="类中的方法和属性"><a href="#类中的方法和属性" class="headerlink" title="类中的方法和属性"></a>类中的方法和属性</h1><p>方法：也就是各类中定义的函数，比如我们定义一个车的类，描述车的函数就是一个方法。  </p>
<p>属性：车的品牌、型号、生产日期等信息就是它的属性.</p>
<h2 id="init-方法："><a href="#init-方法：" class="headerlink" title="__init__方法："></a>__init__方法：</h2><p>无需直接调用，生成实例对象的时候自动调用。<br>“init”的全称是“initialize”，也就是初始化的意思，所以__init__又称构造方法。  </p>
<p>在定义类的时候<strong>init</strong>()方法是必不可少的。 </p>
<blockquote>
<p>(1)双下划线开头的函数为私有函数，不能在类的外部被调用或直接访问；<br>(2)init()，支持带参数的初始化，例如：def init(self,ai_settings,screen)；<br>(3)init()函数的第一个参数必须为self（也可是别的名字），后续参数可自由指定；</p>
</blockquote>
<p>init()这种初始化方法，用来初始化新创建对象的属性，在一个对象被创建以后会立即调用，比如像实例化一个类：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">  class Car():</span><br><span class="line">    def __init__(self,make,model,year):    ###</span><br><span class="line">        self.make &#x3D; make</span><br><span class="line">        self.model &#x3D; model</span><br><span class="line">        self.year &#x3D; year </span><br><span class="line">my_car &#x3D; Car(&#39;aodi&#39;,&#39;A4&#39;,&#39;2010&#39;)</span><br><span class="line">print(my_car.model)           </span><br></pre></td></tr></table></figure>
<p>程序中没有直接调用__init__方法，但make，model，year等属性通过Car()类自动调用了__init__方法，生成了属性。</p>
<h2 id="self参数"><a href="#self参数" class="headerlink" title="self参数"></a>self参数</h2><p>“self”的英文意思很明显，是自己的意思。即实际指的是，类实例对象本身。</p>
<p>同时，由于说到“自己”这个词，都是和相对而言的“其他”而说的；而此处的其他，指的是，类Class，和其他变量，比如局部变量，全局变量等。<br>此处的self，是个对象（Object），是当前类的实例。<br>因此，对应的self.valueName 和 self.function()中的valueName：表示self对象，即实例的变量。与其他的，Class的变量，全局的变量，局部的变量，是相对应的。<br>function：表示是调用的是self对象，即实例的函数。与其他的全局的函数，是相对应的。</p>
<p>因为Python已经规定：函数的第一个参数，就必须是实例对象本身，并且约定俗成，把其名字写为self。因此我们再定义类中的所有函数时必须传入self参数。  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">class Car():</span><br><span class="line">    def __init__(self,make,model,year):    ###</span><br><span class="line">        self.make &#x3D; make</span><br><span class="line">        self.model &#x3D; model</span><br><span class="line">        self.year &#x3D; year </span><br><span class="line">    def get_descriptive_name(self):</span><br><span class="line">        long_name &#x3D; self.year+&#39; &#39;+self.make+&quot; &quot;+self.model</span><br><span class="line">        print(self)                    ###看下self指向哪里</span><br><span class="line">        print(type(self))              ###看下self类型是什么</span><br><span class="line">        return long_name</span><br><span class="line">my_car &#x3D; Car(&#39;aodi&#39;,&#39;A4&#39;,&#39;2010&#39;)</span><br><span class="line">my_car.get_descriptive_name()</span><br></pre></td></tr></table></figure>

<h1 id="pytorch模型构建"><a href="#pytorch模型构建" class="headerlink" title="pytorch模型构建"></a>pytorch模型构建</h1><h2 id="模型总体结构"><a href="#模型总体结构" class="headerlink" title="模型总体结构"></a>模型总体结构</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyNet, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;网络的各层具体结构定义&quot;</span>  </span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;网络各层的输入输出传递&quot;</span> </span><br><span class="line">        </span><br></pre></td></tr></table></figure>
<h2 id="模型定义的具体方法"><a href="#模型定义的具体方法" class="headerlink" title="模型定义的具体方法"></a>模型定义的具体方法</h2><p>假设定义一个网络，输入为3通道大小为28*28的图片，经过以下网络输出10分类softmax分类结果：<br>卷积-&gt;卷积-&gt;ReLu激活-&gt;池化-&gt;卷积-&gt;卷积-&gt;ReLu激活-&gt;全连接-&gt;全连接-&gt;softmax</p>
<h3 id="方法一："><a href="#方法一：" class="headerlink" title="方法一："></a>方法一：</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyNet, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.conv3 = nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv4 = nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        self.dense1 = nn.Linear(<span class="number">2048</span>, <span class="number">128</span>)</span><br><span class="line">        self.dense2 = nn.Linear(<span class="number">128</span>, <span class="number">10</span>)</span><br><span class="line">        self.softmax = nn.Softmax()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.conv1(x)  <span class="comment"># shape = 32,26,26</span></span><br><span class="line">        x = self.conv2(x)  <span class="comment"># shape = 32,24,24</span></span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.pool(x)  <span class="comment"># shape = 32,12,12</span></span><br><span class="line">        x = self.conv3(x)  <span class="comment"># shape = 32,10,10</span></span><br><span class="line">        x = self.conv4(x)  <span class="comment"># shape = 32,8,8</span></span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)  <span class="comment"># shape = 2048</span></span><br><span class="line">        x = self.dense1(x)  <span class="comment"># shape = 128</span></span><br><span class="line">        x = self.dense2(x)  <span class="comment"># shape = 10</span></span><br><span class="line">        output = self.softmax(x)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">打印模型：</span><br><span class="line">&gt;&gt;model &#x3D; MyNet()</span><br><span class="line">&gt;&gt;print(model)</span><br><span class="line"></span><br><span class="line">MyNet(</span><br><span class="line">  (conv1): Conv2d(3, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1))</span><br><span class="line">  (conv2): Conv2d(32, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1))</span><br><span class="line">  (relu): ReLU(inplace&#x3D;True)</span><br><span class="line">  (pool): MaxPool2d(kernel_size&#x3D;2, stride&#x3D;2, padding&#x3D;0, dilation&#x3D;1, ceil_mode&#x3D;False)</span><br><span class="line">  (conv3): Conv2d(32, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1))</span><br><span class="line">  (conv4): Conv2d(32, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1))</span><br><span class="line">  (dense1): Linear(in_features&#x3D;2048, out_features&#x3D;128, bias&#x3D;True)</span><br><span class="line">  (dense2): Linear(in_features&#x3D;128, out_features&#x3D;10, bias&#x3D;True)</span><br><span class="line">  (softmax): Softmax(dim&#x3D;None)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>其中类似于ReLu激活函数的层在网络中有两种写法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"></span><br><span class="line">class MyNet1(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(MyNet1, self).__init__()</span><br><span class="line">        self.relu &#x3D; nn.ReLU(inplace&#x3D;True)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x &#x3D; self.relu(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class MyNet2(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(MyNet2, self).__init__()</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x &#x3D; F.relu(x,inplace&#x3D;True)</span><br></pre></td></tr></table></figure>
<p>在如上网络中，MyNet1与MyNet2实现的结果是一致的，但是可以看到将ReLU层添加到网络有两种不同的实现，即nn.ReLU和F.ReLU两种实现方法。  </p>
<p>其中nn.ReLU作为一个层结构，必须添加到nn.Module容器中才能使用，而F.ReLU则作为一个函数调用，看上去作为一个函数调用更方便更简洁。具体使用哪种方式，取决于编程风格。在PyTorch中,nn.X都有对应的函数版本F.X，但是并不是所有的F.X均可以用于forward或其它代码段中，因为当网络模型训练完毕时，在存储model时，在forward中的F.X函数中的参数是无法保存的。也就是说，在forward中，使用的F.X函数一般均没有状态参数，比如F.ReLU，F.avg_pool2d等，均没有参数，它们可以用在任何代码片段中。</p>
<h3 id="方法二："><a href="#方法二：" class="headerlink" title="方法二："></a>方法二：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line"></span><br><span class="line">class MyNet3(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(MyNet3, self).__init__()</span><br><span class="line">        self.layer1 &#x3D; nn.Sequential(</span><br><span class="line">        	nn.Conv2d(3, 32, 3, 1),</span><br><span class="line">        	nn.Conv2d(32, 32, 3, 1),</span><br><span class="line">        	nn.ReLU(inplace&#x3D;True),</span><br><span class="line">        	nn.MaxPool2d(2))</span><br><span class="line">        self.layer2 &#x3D; nn.Sequential(</span><br><span class="line">        	nn.Conv2d(32, 32, 3, 1),</span><br><span class="line">        	nn.Conv2d(32, 32, 3, 1))</span><br><span class="line">        self.fc &#x3D; nn.Sequential(</span><br><span class="line">        	nn.Linear(2048, 128),</span><br><span class="line">        	nn.Linear(128, 10),</span><br><span class="line">        	nn.Softmax())</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x &#x3D; self.layer1(x)</span><br><span class="line">        x &#x3D; self.layer2(x)</span><br><span class="line">        x &#x3D; x.view(x.size(0), -1)</span><br><span class="line">        output &#x3D; self.fc(x)</span><br><span class="line">        return output</span><br></pre></td></tr></table></figure>
<p>这种方法利用torch.nn.Sequential（）容器进行快速搭建，模型的各层被顺序添加到容器中。这种方法将多个层放在一起作为一个块运行，比较直观，缺点是每层的编号是默认的阿拉伯数字，不易区分。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">打印模型：</span><br><span class="line">&gt;&gt;model &#x3D; MyNet3()</span><br><span class="line">&gt;&gt;print(model)</span><br><span class="line"></span><br><span class="line">MyNet3(</span><br><span class="line">  (layer1): Sequential(</span><br><span class="line">    (0): Conv2d(3, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1))</span><br><span class="line">    (1): Conv2d(32, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1))</span><br><span class="line">    (2): ReLU(inplace&#x3D;True)</span><br><span class="line">    (3): MaxPool2d(kernel_size&#x3D;2, stride&#x3D;2, padding&#x3D;0, dilation&#x3D;1, ceil_mode&#x3D;False)</span><br><span class="line">  )</span><br><span class="line">  (layer2): Sequential(</span><br><span class="line">    (0): Conv2d(32, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1))</span><br><span class="line">    (1): Conv2d(32, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1))</span><br><span class="line">  )</span><br><span class="line">  (fc): Sequential(</span><br><span class="line">    (0): Linear(in_features&#x3D;2048, out_features&#x3D;128, bias&#x3D;True)</span><br><span class="line">    (1): Linear(in_features&#x3D;128, out_features&#x3D;10, bias&#x3D;True)</span><br><span class="line">    (2): Softmax(dim&#x3D;None)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="方法三："><a href="#方法三：" class="headerlink" title="方法三："></a>方法三：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line"></span><br><span class="line">class MyNet4(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(MyNet4, self).__init__()</span><br><span class="line">        self.layer1 &#x3D; nn.Sequential()</span><br><span class="line">        self.layer1.add_module(&#39;conv1&#39;,nn.Conv2d(3, 32, 3, 1))</span><br><span class="line">        self.layer1.add_module(&#39;conv2&#39;,nn.Conv2d(32, 32, 3, 1))</span><br><span class="line">        self.layer1.add_module(&#39;relu1&#39;,nn.ReLU(inplace&#x3D;True))</span><br><span class="line">        self.layer1.add_module(&#39;pool1&#39;,nn.MaxPool2d(2))</span><br><span class="line"></span><br><span class="line">        self.layer2 &#x3D; nn.Sequential()</span><br><span class="line">        self.layer2.add_module(&#39;conv3&#39;,nn.Conv2d(32, 32, 3, 1))</span><br><span class="line">        self.layer2.add_module(&#39;conv4&#39;,nn.Conv2d(32, 32, 3, 1))</span><br><span class="line">        self.layer2.add_module(&#39;relu2&#39;,nn.ReLU(inplace&#x3D;True))</span><br><span class="line"></span><br><span class="line">        self.fc &#x3D; nn.Sequential()</span><br><span class="line">        self.fc.add_module(&#39;fc1&#39;,nn.Linear(2048, 128))</span><br><span class="line">        self.fc.add_module(&#39;fc1&#39;,nn.Linear(128, 10))</span><br><span class="line">        self.fc.add_module(&#39;softmax&#39;,nn.Softmax())</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x &#x3D; self.layer1(x)</span><br><span class="line">        x &#x3D; self.layer2(x)</span><br><span class="line">        x &#x3D; x.view(x.size(0), -1)</span><br><span class="line">        output &#x3D; self.fc(x)</span><br><span class="line">        return output</span><br></pre></td></tr></table></figure>
<p>这种方法是对第二种方法的改进：通过add_module()添加每一层，并且为每一层增加了一个单独的名字。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">打印模型：</span><br><span class="line">&gt;&gt;model &#x3D; MyNet4()</span><br><span class="line">&gt;&gt;print(model)</span><br><span class="line"></span><br><span class="line">MyNet4(</span><br><span class="line">  (layer1): Sequential(</span><br><span class="line">    (conv1): Conv2d(3, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1))</span><br><span class="line">    (conv2): Conv2d(32, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1))</span><br><span class="line">    (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">    (pool1): MaxPool2d(kernel_size&#x3D;2, stride&#x3D;2, padding&#x3D;0, dilation&#x3D;1, ceil_mode&#x3D;False)</span><br><span class="line">  )</span><br><span class="line">  (layer2): Sequential(</span><br><span class="line">    (conv3): Conv2d(32, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1))</span><br><span class="line">    (conv4): Conv2d(32, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1))</span><br><span class="line">    (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">  )</span><br><span class="line">  (fc): Sequential(</span><br><span class="line">    (fc1): Linear(in_features&#x3D;128, out_features&#x3D;10, bias&#x3D;True)</span><br><span class="line">    (softmax): Softmax(dim&#x3D;None)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="方法四："><a href="#方法四：" class="headerlink" title="方法四："></a>方法四：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">from collections import OrderedDict</span><br><span class="line"></span><br><span class="line">class MyNet5(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(MyNet5, self).__init__()</span><br><span class="line">        self.layer1 &#x3D; nn.Sequential(</span><br><span class="line">            OrderedDict([</span><br><span class="line">                (&#39;conv1&#39;, nn.Conv2d(3, 32, 3, 1)),</span><br><span class="line">                (&#39;conv2&#39;, nn.Conv2d(32, 32, 3, 1)),</span><br><span class="line">                (&#39;relu1&#39;, nn.ReLU(inplace&#x3D;True)),</span><br><span class="line">                (&#39;pool1&#39;, nn.MaxPool2d(2))</span><br><span class="line">            ]))</span><br><span class="line"></span><br><span class="line">        self.layer2 &#x3D; nn.Sequential(</span><br><span class="line">            OrderedDict([</span><br><span class="line">                (&#39;conv3&#39;, nn.Conv2d(32, 32, 3, 1)),</span><br><span class="line">                (&#39;conv4&#39;, nn.Conv2d(32, 32, 3, 1)),</span><br><span class="line">                (&#39;relu2&#39;, nn.ReLU(inplace&#x3D;True))</span><br><span class="line">            ]))</span><br><span class="line"></span><br><span class="line">        self.fc &#x3D; nn.Sequential(</span><br><span class="line">            OrderedDict([</span><br><span class="line">                (&#39;fc1&#39;, nn.Linear(2048, 128)),</span><br><span class="line">                (&#39;fc1&#39;, nn.Linear(128, 10)),</span><br><span class="line">                (&#39;softmax&#39;, nn.Softmax())</span><br><span class="line">            ]))</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x &#x3D; self.layer1(x)</span><br><span class="line">        x &#x3D; self.layer2(x)</span><br><span class="line">        x &#x3D; x.view(x.size(0), -1)</span><br><span class="line">        output &#x3D; self.fc(x)</span><br><span class="line">        return output</span><br></pre></td></tr></table></figure>
<p>这个方法是第三种方法的另外一种写法，通过字典的形式添加每一层，并且设置单独的层名称。  </p>
<p>OrderedDict是python的有序字典，使用OrderedDict会根据放入元素的先后顺序进行排序，所以输出的值是排好序的。 OrderedDict对象的字典对象如果其顺序不同，那么Python也会把他们当做是两个不同的对象。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">打印模型：</span><br><span class="line">&gt;&gt;model &#x3D; MyNet5()</span><br><span class="line">&gt;&gt;print(model)</span><br><span class="line"></span><br><span class="line">MyNet5(</span><br><span class="line">  (layer1): Sequential(</span><br><span class="line">    (conv1): Conv2d(3, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1))</span><br><span class="line">    (conv2): Conv2d(32, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1))</span><br><span class="line">    (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">    (pool1): MaxPool2d(kernel_size&#x3D;2, stride&#x3D;2, padding&#x3D;0, dilation&#x3D;1, ceil_mode&#x3D;False)</span><br><span class="line">  )</span><br><span class="line">  (layer2): Sequential(</span><br><span class="line">    (conv3): Conv2d(32, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1))</span><br><span class="line">    (conv4): Conv2d(32, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1))</span><br><span class="line">    (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">  )</span><br><span class="line">  (fc): Sequential(</span><br><span class="line">    (fc1): Linear(in_features&#x3D;128, out_features&#x3D;10, bias&#x3D;True)</span><br><span class="line">    (softmax): Softmax(dim&#x3D;None)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h1 id="模型的参数共享"><a href="#模型的参数共享" class="headerlink" title="模型的参数共享"></a>模型的参数共享</h1><p>假设只有3层卷积的模型有以下两种写法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line"></span><br><span class="line">class MyNet6(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(MyNet6, self).__init__()</span><br><span class="line">        self.conv1 &#x3D; nn.Conv2d(3, 32, 3, 1)</span><br><span class="line">        self.conv2 &#x3D; nn.Conv2d(32, 32, 3, 1)</span><br><span class="line">        self.conv3 &#x3D; nn.Conv2d(32, 32, 3, 1)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x &#x3D; self.conv1(x)</span><br><span class="line">        x &#x3D; self.conv2(x)</span><br><span class="line">        x &#x3D; self.conv3(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class MyNet7(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(MyNet7, self).__init__()</span><br><span class="line">        self.conv1 &#x3D; nn.Conv2d(3, 32, 3, 1)</span><br><span class="line">        self.conv2 &#x3D; nn.Conv2d(32, 32, 3, 1)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x &#x3D; self.conv1(x)</span><br><span class="line">        x &#x3D; self.conv2(x)</span><br><span class="line">        x &#x3D; self.conv2(x)</span><br><span class="line">        return x</span><br></pre></td></tr></table></figure>
<p>以输入为28*28的3通道图片作为输入，打印模型的参数。pytorch中模型的参数统计输出方法有以下两种:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">from torchsummary import summary</span><br><span class="line"></span><br><span class="line">model &#x3D; MyNet6()</span><br><span class="line">summary(model,(3,28,28))</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">def count_parameters(model):</span><br><span class="line">    return sum(p.numel() for p in model.parameters() if p.requires_grad)</span><br><span class="line"></span><br><span class="line">print(&#39;parameters_count:&#39;,count_parameters(model))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>MyNet6的参数打印两种结果:<br><img src="1.png" loading="lazy">  </p>
<p>MyNet7的参数打印两种结果：<br><img src="2.png" loading="lazy"><br>此时会发现summary方法输出的参数符合计算，但parameters_count方法输出的参数量少了。  </p>
<p>为什么会出现这种问题？要想找到原因肯定是要先了解网络是怎么构建的，从网络构建可以看出，这个网络只初始化了两个卷积层对象——conv1和conv2，然后在网络构建时（forward里面），重复调用了conv2，这样做是因为：根据pytorch官方的教程，这样可以实现参数共享，也就是Conv2d-2 和Conv2d-3 层共享了conv2的参数。也就是其实这里只用了一个卷积层的参数，所以parameters_count 计算的是对的，但是torchsummary为什么计算成了19392？ 那是因为torchsummary 计算时是先把层结构打印下来，然后再统计对各个层的参数求和，这样一来，它不会区分conv2d-2和conv2d-3里面的参数是否相同，只是根据结构都打印且统计了出来。所以在遇到参数共享的时候，torchsummary统计的是不正确的。<br>那parameters_count计算出来的结果一定对吗？看下面模型：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">class MyNet8(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(MyNet8, self).__init__()</span><br><span class="line">        self.conv1 &#x3D; nn.Conv2d(3, 32, 3, 1)</span><br><span class="line">        self.conv2 &#x3D; nn.Conv2d(32, 32, 3, 1)</span><br><span class="line">        self.conv3 &#x3D; nn.Conv2d(32, 32, 3, 1)</span><br><span class="line">        self.conv4 &#x3D; nn.Conv2d(32, 64, 3, 1)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x &#x3D; self.conv1(x)</span><br><span class="line">        x &#x3D; self.conv2(x)</span><br><span class="line">        x &#x3D; self.conv3(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这里初始化了一个conv4的层，但是在forward函数里没有使用，也就是MyNet8跟MyNet6的结构是等价的，理论参数应该同样为19392。  </p>
<p>MyNet8的参数打印两种结果：<br><img src="3.png" loading="lazy"><br>可以发现这个时候，parameters_count方法出现了错误。因为在MyNet8里多初始化了conv4,即使没有在forward里面调用，但是它也算在的model.parameters()里面。  </p>
<p>因此，如果没有共享参数的情况出现尽量用torchsummary来计算，如果出现了共享参数的情况，那就用parameters_count的计算方式，这个时候要注意尽量将没有用到的层对象注释干净，这样才能计算出正确的参数来。</p>
<h1 id="ModuleList和Sequencial"><a href="#ModuleList和Sequencial" class="headerlink" title="ModuleList和Sequencial"></a>ModuleList和Sequencial</h1><h2 id="nn-ModuleList"><a href="#nn-ModuleList" class="headerlink" title="nn.ModuleList"></a>nn.ModuleList</h2><p>ModuleList功能类似于python中的列表（list），可以把nn.Conv2d，nn.Linear等加入到里面去，但与列表不同的是，加入到nn.ModuleList中的模块会注册到网络中，并且里面的层参数也会添加到网络中。举例如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">class net1(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(net1, self).__init__()</span><br><span class="line">        self.linears &#x3D; nn.ModuleList([nn.Linear(10,10) for i in range(2)])</span><br><span class="line">    def forward(self, x):</span><br><span class="line">        for m in self.linears:</span><br><span class="line">            x &#x3D; m(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">net &#x3D; net1()</span><br><span class="line">print(net)</span><br><span class="line"># net1(</span><br><span class="line">#   (modules): ModuleList(</span><br><span class="line">#     (0): Linear(in_features&#x3D;10, out_features&#x3D;10, bias&#x3D;True)</span><br><span class="line">#     (1): Linear(in_features&#x3D;10, out_features&#x3D;10, bias&#x3D;True)</span><br><span class="line">#   )</span><br><span class="line"># )</span><br><span class="line"></span><br><span class="line">for param in net.parameters():</span><br><span class="line">    print(type(param.data), param.size())</span><br><span class="line"># &lt;class &#39;torch.Tensor&#39;&gt; torch.Size([10, 10])</span><br><span class="line"># &lt;class &#39;torch.Tensor&#39;&gt; torch.Size([10])</span><br><span class="line"># &lt;class &#39;torch.Tensor&#39;&gt; torch.Size([10, 10])</span><br><span class="line"># &lt;class &#39;torch.Tensor&#39;&gt; torch.Size([10])</span><br></pre></td></tr></table></figure>
<p>这是一个包含两个全连接层的简单网络，打印网络结构和参数可以看到每层的权值(weights)和偏置(bias)都在网络之内。接下来使用列表：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">class net2(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(net2, self).__init__()</span><br><span class="line">        self.linears &#x3D; [nn.Linear(10,10) for i in range(2)]</span><br><span class="line">    def forward(self, x):</span><br><span class="line">        for m in self.linears:</span><br><span class="line">            x &#x3D; m(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">net &#x3D; net2()</span><br><span class="line">print(net)</span><br><span class="line"># net2()</span><br><span class="line">print(list(net.parameters()))</span><br><span class="line"># []</span><br></pre></td></tr></table></figure>
<p>使用 Python 的 list 添加的全连接层和它们的 parameters 并没有自动注册到网络中。虽然还是可以使用 forward 来计算输出结果，但是如果用 net2 实例化的网络进行训练的时候，因为这些层的 parameters 不在整个网络之中，所以其网络参数也不会被更新，也就是无法训练。  </p>
<p>除此之外，ModuleList中添加的层没有先后顺序，最终顺序是按照forward函数里面的执行顺序决定，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">class net3(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(net3, self).__init__()</span><br><span class="line">        self.linears &#x3D; nn.ModuleList([nn.Linear(10,20), nn.Linear(20,30), nn.Linear(5,10)])</span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x &#x3D; self.linears[2](x)</span><br><span class="line">        x &#x3D; self.linears[0](x)</span><br><span class="line">        x &#x3D; self.linears[1](x) </span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">net &#x3D; net3()</span><br><span class="line">print(net)</span><br><span class="line"># net3(</span><br><span class="line">#   (linears): ModuleList(</span><br><span class="line">#     (0): Linear(in_features&#x3D;10, out_features&#x3D;20, bias&#x3D;True)</span><br><span class="line">#     (1): Linear(in_features&#x3D;20, out_features&#x3D;30, bias&#x3D;True)</span><br><span class="line">#     (2): Linear(in_features&#x3D;5, out_features&#x3D;10, bias&#x3D;True)</span><br><span class="line">#   )</span><br><span class="line"># )</span><br></pre></td></tr></table></figure>
<p>在ModuleList中，同一个元素（Module、层）也是不能重复使用的，重复使用也会参数共享。</p>
<h2 id="nn-Sequencial"><a href="#nn-Sequencial" class="headerlink" title="nn.Sequencial"></a>nn.Sequencial</h2><p>第二节模型构建的方法中除了第一种是一层层写，其他三种都是利用Sequencial方法。Sequencial方法和keras中的序贯模型很像，与ModuleList不同的是，在模型最终执行时，Sequencial里面的模块（层）会按照顺序执行。并且由于Sequential本身已经实现了forward函数，因此在定义模型的时候，可以不需要forward函数，举例如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">class net5(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(net5, self).__init__()</span><br><span class="line">        self.block &#x3D; nn.Sequential(nn.Conv2d(1,20,5),</span><br><span class="line">                                    nn.ReLU(),</span><br><span class="line">                                    nn.Conv2d(20,64,5),</span><br><span class="line">                                    nn.ReLU())</span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x &#x3D; self.block(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">net &#x3D; net5()</span><br><span class="line">print(net)</span><br><span class="line"># net5(</span><br><span class="line">#   (block): Sequential(</span><br><span class="line">#     (0): Conv2d(1, 20, kernel_size&#x3D;(5, 5), stride&#x3D;(1, 1))</span><br><span class="line">#     (1): ReLU()</span><br><span class="line">#     (2): Conv2d(20, 64, kernel_size&#x3D;(5, 5), stride&#x3D;(1, 1))</span><br><span class="line">#     (3): ReLU()</span><br><span class="line">#   )</span><br><span class="line"># )</span><br></pre></td></tr></table></figure>
<p>上面的方法是按照模型构建方法进行实现，不使用forward可以这么写：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">model1 &#x3D; nn.Sequential(</span><br><span class="line">          nn.Conv2d(1,20,5),</span><br><span class="line">          nn.ReLU(),</span><br><span class="line">          nn.Conv2d(20,64,5),</span><br><span class="line">          nn.ReLU()</span><br><span class="line">        )</span><br><span class="line">print(model1)</span><br><span class="line"># Sequential(</span><br><span class="line">#   (0): Conv2d(1, 20, kernel_size&#x3D;(5, 5), stride&#x3D;(1, 1))</span><br><span class="line">#   (1): ReLU()</span><br><span class="line">#   (2): Conv2d(20, 64, kernel_size&#x3D;(5, 5), stride&#x3D;(1, 1))</span><br><span class="line">#   (3): ReLU()</span><br><span class="line"># )</span><br></pre></td></tr></table></figure>
<p>这种方法省去了定义模型类的过程，这种写法构建的模型与上面net5是等价的。在不需要添加一些其他处理函数（比如全连接层之前的flatten操作：x = x.view(x.size(0), -1)）的情况下可以这么写。  </p>
<p>一般来说，nn.Sequential 的用法是来组成卷积块 (block)，然后像拼积木一样把不同的 block 拼成整个网络，让代码更简洁直观，更加结构化。</p>
<h2 id="nn-ModuleList和nn-Sequential使用情景"><a href="#nn-ModuleList和nn-Sequential使用情景" class="headerlink" title="nn.ModuleList和nn.Sequential使用情景"></a>nn.ModuleList和nn.Sequential使用情景</h2><p>场景一，有的时候网络中有很多相似或者重复的层，一般会考虑用 for 循环来创建它们，而不是一行一行地写，比如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">layers &#x3D; [nn.Linear(10, 10) for i in range(3)]</span><br></pre></td></tr></table></figure>
<p>这个时候会想到用ModuleList：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">class net6(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(net6, self).__init__()</span><br><span class="line">        self.linears &#x3D; nn.ModuleList([nn.Linear(10, 10) for i in range(3)])</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        for layer in self.linears:</span><br><span class="line">            x &#x3D; layer(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">net &#x3D; net6()</span><br><span class="line">print(net)</span><br><span class="line"># net6(</span><br><span class="line">#   (linears): ModuleList(</span><br><span class="line">#     (0): Linear(in_features&#x3D;10, out_features&#x3D;10, bias&#x3D;True)</span><br><span class="line">#     (1): Linear(in_features&#x3D;10, out_features&#x3D;10, bias&#x3D;True)</span><br><span class="line">#     (2): Linear(in_features&#x3D;10, out_features&#x3D;10, bias&#x3D;True)</span><br><span class="line">#   )</span><br><span class="line"># )</span><br></pre></td></tr></table></figure>
<p>这种也可以使用Sequential：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">class net7(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(net7, self).__init__()</span><br><span class="line">        self.linear_list &#x3D; [nn.Linear(10, 10) for i in range(3)]</span><br><span class="line">        self.linears &#x3D; nn.Sequential(*self.linears_list)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        self.x &#x3D; self.linears(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">net &#x3D; net7()</span><br><span class="line">print(net)</span><br><span class="line"># net7(</span><br><span class="line">#   (linears): Sequential(</span><br><span class="line">#     (0): Linear(in_features&#x3D;10, out_features&#x3D;10, bias&#x3D;True)</span><br><span class="line">#     (1): Linear(in_features&#x3D;10, out_features&#x3D;10, bias&#x3D;True)</span><br><span class="line">#     (2): Linear(in_features&#x3D;10, out_features&#x3D;10, bias&#x3D;True)</span><br><span class="line">#   )</span><br><span class="line"># )</span><br></pre></td></tr></table></figure>
<p>需要注意的是第5行的*操作符。它的作用是把一个 list 拆开成一个个独立的元素。但是这个 list 里面的模块必须是按照想要的顺序来进行排列。  </p>
<p>场景二，当需要之前层的信息的时候，比如 ResNets 中的 shortcut 结构，或者是像 FCN 中用到的 skip architecture 之类的，当前层的结果需要和之前层中的结果进行融合，一般使用 ModuleList 比较方便，举例如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">class net8(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(net8, self).__init__()</span><br><span class="line">        self.linears &#x3D; nn.ModuleList([nn.Linear(10, 20), </span><br><span class="line">                                      nn.Linear(20, 30), </span><br><span class="line">                                      nn.Linear(30, 50)])</span><br><span class="line">        self.trace &#x3D; []</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        for layer in self.linears:</span><br><span class="line">            x &#x3D; layer(x)</span><br><span class="line">            self.trace.append(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">net &#x3D; net8()</span><br><span class="line">input  &#x3D; torch.randn(32, 10) # input batch size: 32</span><br><span class="line">output &#x3D; net(input)</span><br><span class="line">for each in net.trace:</span><br><span class="line">    print(each.shape)</span><br><span class="line"># torch.Size([32, 20])</span><br><span class="line"># torch.Size([32, 30])</span><br><span class="line"># torch.Size([32, 50])</span><br></pre></td></tr></table></figure>
<p>这里使用了一个列表trace来储存网络每层的输出结果，这样如果以后的层要用的话，就可以很方便地调用了。</p>
</div><ul class="post-copyright"><li class="post-copyright-author"><strong>Post author: </strong>符玲玲</li><li class="post-copyright-link"><strong>Post link: </strong><a href="https://lingo1101.github.io/2020/10/24/learn/Deeplearn/pytorch/" title="pytorch构建模型">https://lingo1101.github.io/2020/10/24/learn/Deeplearn/pytorch/</a></li><li class="post-copyright-license"><strong>Copyright Notice: </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><svg class="icon"><use xlink:href="#icon-creative-commons-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-by-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-nc-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-sa-line"></use></svg></a> unless stating additionally.</li></ul></section></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/2020/10/24/learn/skills/golong/" rel="prev" title="go语言"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-left-s-line"></use></svg><span class="post-nav-text">go语言</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/2020/10/24/travel/Sanya/" rel="next" title="三亚"><span class="post-nav-text">三亚</span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-right-s-line"></use></svg></a></div></div></div><div id="comment"><div class="comment-tooltip text-center"><span>点击按钮跳转 GitHub Issues 评论。</span><br><span>若没有本文 Issue，您可以使用 Comment 模版新建。</span><br><a class="hty-button hty-button--raised" id="github-issues" target="_blank" rel="noopener" href="https://github.com/Lingo1101/Lingo1101.github.io/issues?q=is:issue+pytorch构建模型">GitHub Issues</a></div></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 – 2020 </span><span class="with-love" id="animate"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-cloud-line"></use></svg></span><span class="author"> 符玲玲</span></div><div class="powered"><span>Powered by <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> v5.2.0</span><span class="footer-separator">|</span><span>Theme - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.2.0</span></div></footer><a class="hty-icon-button" id="goUp" aria-label="back-to-top" href="#"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-up-s-line"></use></svg><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a></div><script defer src="/js/utils.js"></script><script defer src="/js/hexo-theme-yun.js"></script></body></html>